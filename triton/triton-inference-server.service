[Unit]
Description=Triton Inference Server
Requires=docker.service
ConditionPathExists=!/var/tmp/container-images

[Service]
Type=simple
Restart=on-failure
ExecStart=/usr/bin/docker run --rm --name triton --runtime nvidia --volume /usr/local/share/triton/models:/models --publish 127.0.0.1:9000:8000/tcp --publish 9001:8001/tcp harbor-registry.whoi.edu/nvidia/tritonserver:22.02-pyt-python-py3 tritonserver --model-repository /models --strict-model-config=false
ExecStop=/usr/bin/docker stop triton

[Install]
